---
title: "NYC LL84 ML"
author: "Eden Axelrad"
date: "7/17/2022"
output:
  pdf_document: 
    toc: yes
    toc_depth: 3
urlcolor: blue
---

## SECTION 1: Introduction and Overview

- Data retrieved from NYC Open Data as part of Local Law 84 (LL84)
- Data described as Data and metrics on water and energy consumption in privately owned buildings over 25,000 ft2 and in City-owned buildings over 10,000 ft2.
- R script used to retrieve, wrangle, and clean the data frame used in this main report can be found in the repo, or accessed via [this link](https://github.com/edenaxe/NYC-LL84-ML/blob/main/Get%20Data.R). 


## SECTION 2: Setup

In this section we set up the initial data set

```{r packages, echo=FALSE, warning=FALSE, message=FALSE}

# Load required packages
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(caret)) install.packages('caret')
if (!require(rpart)) install.packages('rpart')
if (!require(randomForest)) install.packages('randomForest')
if (!require(scales)) install.packages('scales')
if (!require(knitr)) install.packages('knitr')

library(tidyverse)
library(caret)
library(rpart)
library(randomForest)

```

```{r setup, warning=FALSE}

# Load the cleaned up data for reporting years 2016 through 2019
NYC_LL84 <- read.csv("Data/NYC_LL84_Clean.csv") 

# Create our test and train data for machine learning from reporting years 2016-18
NYC_LL84_ML <- NYC_LL84 %>%
  filter(year %in% c("2016", "2017", "2018")) %>%
  select(year, primary_property_type, year_built,
         number_of_buildings, energy_star_score, property_gfa, parking_gfa,
         leed_project, CDD, HDD, MeanMaxTemp, MeanMinTemp, site_eui)

# Create the validation set to be used in the very end 
NYC_LL84_Validation <- NYC_LL84 %>%
  filter(year == "2019") %>%
  select(year, primary_property_type, year_built,
         number_of_buildings, energy_star_score, property_gfa, parking_gfa,
         leed_project, CDD, HDD, MeanMaxTemp, MeanMinTemp, site_eui)

```

## SECTION 3: Methods / Analysis

This section explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and modeling approach

### SECTION 3.1: Exploratory Analysis

```{r exploratory, warning=FALSE, message=FALSE}

# Summary of data frame
glimpse(NYC_LL84_ML)

summary(NYC_LL84_ML$site_eui)

# EUI and GHG by Property Type
NYC_LL84_ML %>%
  group_by("Property Type" = primary_property_type) %>%
  summarise("Site EUI (kBtu/sqft)" = round(mean(site_eui), 
                                           digits = 1), 
            Count = n())  %>%
  arrange(desc(Count)) %>%
  knitr::kable()

# Explore EUI 
summary(NYC_LL84_ML$site_eui)

NYC_LL84_ML %>%
  mutate(primary_property_type = fct_reorder(primary_property_type, 
                                             site_eui)) %>%
  ggplot(aes(x = primary_property_type, 
             y = site_eui, 
             fill = primary_property_type)) +
  geom_boxplot(size = .3, outlier.size = 0.3) +
  labs(x = "Primary Property Type",
       y = "Site Energy Use Intensity (kBtu/sqft)",
       title = "Site EUI by Primary Property Type") +
  theme(legend.position = "none") +
  coord_flip()

```

```{r}

# Size of building versus EUI
summary(NYC_LL84_ML$property_gfa)

NYC_LL84_ML %>%
  mutate(size_quartile = case_when(
    `property_gfa` > 113686 ~ 4,
    `property_gfa` > 63968 & `property_gfa` <= 113686 ~ 3,
    `property_gfa` > 40760 & `property_gfa` <= 63968 ~ 2,
    `property_gfa` <= 40760 ~ 1)) %>%
  group_by(size_quartile) %>%
  summarise("Site EUI (kBtu/sqft)" = round(mean(site_eui), 
                                           digits = 1),
            Count = n())  %>%
  arrange(size_quartile) %>%
  knitr::kable()

```

```{r cor, warning=FALSE}

# Look at correlation by feature for total GHG emissions 
# Examine EUI data and check for outliers
NYC_LL84_ML %>%
  select_if(., is.numeric) %>%
  cor() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Feature") %>%
  select(Feature, site_eui) %>%
  drop_na() %>%
  arrange(desc(abs(site_eui))) %>%
  knitr::kable() 

```

insert text

### SECTION 3.2: Actual Methods

insert text

```{r, warning=FALSE}

# Partition the data in to a test set with 20% and train set with 80%
# Set seed to 92 for reproducing results  
set.seed(92, sample.kind = "Rounding")
test_index <- createDataPartition(NYC_LL84_ML$site_eui, 
                                  times = 1, p = 0.2, list = FALSE)
test_set <- NYC_LL84_ML %>% slice(test_index)
train_set <- NYC_LL84_ML %>% slice(-test_index)

```

#####  Method #1: Naive Model

```{r naive}

# Start off with a naive model that uses the average rating to predict movie ratings
mu_hat <- mean(train_set$site_eui)

# Save the model RMSE
rmse_naive <- RMSE(pred = mu_hat,
                   obs = test_set$site_eui)

```

##### Method #2: Mean EUI by Property Type

In the field, this is a common way of improving an estimate for EUI or GHG emissions. For this reason, I wanted to include it alongside the other methods. To execute it, simply find the average EUI by property type and left join with the test set. The assumption here is that property type is an easy way to break down the data in to categories that significantly impact EUI. 

```{r prop avg}

# Generate predicted Site EUI using the test set
pred_eui_avg <- left_join(test_set, 
                          train_set %>%
                            group_by(primary_property_type) %>%
                            summarize(pred_avg_eui = mean(site_eui)),
                          by = "primary_property_type") %>%
  .$pred_avg_eui

# Save the model RMSE
rmse_eui_avg <- RMSE(pred_eui_avg, 
                     obs = test_set$site_eui, 
                     na.rm = TRUE)

```

##### Method #3: Simple Linear Regression Using ENERGY STAR Score

```{r lr}

# Create a linear model
set.seed(92, sample.kind = "Rounding")
model_lr <- train(site_eui ~ energy_star_score, 
               data = train_set, 
               method = "lm")

# Generate predicted Site EUI using the test set
pred_lr <- predict(model_lr, 
                newdata = test_set %>%
                  select(site_eui, energy_star_score))

# Save the model RMSE
rmse_lr <- RMSE(pred = pred_lr, 
                    obs = test_set$site_eui, 
                    na.rm = TRUE)

```

##### Method #4: Multiple Linear Regression with all variables

```{r mlr, warning=FALSE}

# Resampling: Cross-Validated (10 fold, repeated 3 times)
# No pre-processing
set.seed(92, sample.kind = "Rounding")
model_mlr <- train(site_eui ~ ., 
                   trControl = trainControl(method = "repeatedcv", repeats = 3),
                   data = train_set, 
                   method = "lm")

# Generate predicted Site EUI using the test set
pred_mlr <- predict(model_mlr, 
                    newdata = test_set)

# Save the model RMSE
rmse_mlr <- RMSE(pred = pred_mlr, 
                    obs = test_set$site_eui, 
                    na.rm = TRUE)

```


##### Method #5: Classification and Regression Tree (CART)

```{r cart, warning=FALSE}

# Need to add CART to packages list
set.seed(92, sample.kind = "Rounding")
model_rpart <- train(site_eui ~ ., 
                     data = train_set, 
                     trControl = trainControl(method = "repeatedcv", repeats = 3),
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(.00001, .001, .0001)))

model_rpart

# Generate predicted Site EUI using the test set
pred_rpart <- predict(model_rpart, 
                      newdata = test_set)

# Save the model RMSE
rmse_rpart <- RMSE(pred = pred_rpart, 
                   obs = test_set$site_eui, 
                   na.rm = TRUE)

ggplot(model_rpart, highlight = TRUE) +
  geom_text(position = position_nudge(x = 0.0001), 
            aes(label = ifelse(cp == model_rpart$bestTune[[1]], 
                               round(min(model_rpart$results$RMSE), digits = 2), ""))) +
  labs(title = "Optimal Complexity Paramter for CART Model") 

```

##### Method #6: Random Forest

```{r rf tune, warning=FALSE}

# Create a sample of the train set in order to speed up the training and tuning process
set.seed(92, sample.kind = "Rounding")
sample_index <- sample(nrow(train_set), 2000)
sample_set <- train_set %>% slice(sample_index)


# Begin with sample set, default parameters and minimal load
model_rf_default <- train(site_eui ~ ., 
                   data = sample_set,
                   method = "rf", 
                   ntree = 100,
                   trControl = trainControl(method = "cv", number = 3))

model_rf_default


# Find number of optimal trees to include in the model
ntrees <- c(100, 250, 500, 750, 1000)

tree_test <- sapply(ntrees, function(ntrees) {
  set.seed(92, sample.kind = "Rounding")
  model_rf <- train(site_eui ~ ., 
                    data = sample_set,
                    method = "rf", 
                    ntree = ntrees,
                    trControl = trainControl(method = "cv", number = 3),
                    tuneGrid = data.frame(mtry = model_rf_default$bestTune[[1]]),
                    nSamp = 750)
  min(model_rf$results$RMSE)
})

tree_results <- data.frame("ntrees" = ntrees, 
                           "rmse" = tree_test) 

```

```{r rf train}

# Create the actual rf model using our tuning parameters
# WARNING - Takes quite a long time to execute...
set.seed(92, sample.kind = "Rounding")
model_rf <- train(site_eui ~ ., 
                  data = train_set,
                  method = "rf", 
                  ntree = tree_results %>% slice_min(order_by = rmse) %>% .$ntrees,
                  tuneGrid = data.frame(mtry = model_rf_default$bestTune[[1]]),
                  trControl = trainControl(method = "cv", number = 3))

model_rf

# Generate predicted Site EUI using the test set
pred_rf <- predict(model_rf, 
                   newdata = test_set)

rmse_rf <- RMSE(pred = pred_rf, 
                    obs = test_set$site_eui, 
                    na.rm = TRUE)

```

### SECTION 3.3: Validation

```{r val}

# Generate predicted Site EUI using the test set
pred_validation <- predict(model_rf, 
                      newdata = NYC_LL84_Validation)

# Save the model RMSE
rmse_validation <- RMSE(pred = pred_validation, 
                   obs = NYC_LL84_Validation$site_eui, 
                   na.rm = TRUE)

```

## SECTION 4: Results

Comparing all models against the actual EUI values for reporting year 2019.

```{r results, warning=FALSE, message=FALSE}

Results <- cbind.data.frame(Actual = test_set$site_eui, 
                            `Naive Model` = mu_hat,
                            `Property Type Averages` = pred_eui_avg,
                            `Linear Regression` = pred_lr, 
                            `Multiple Linear Regression` = pred_mlr, 
                            `CART` = pred_rpart,
                            `Random Forest` = pred_rf) %>%
  pivot_longer(cols = 2:7,
               values_to = "Predicted",
               names_to = "Model") %>%
  mutate(Residual = Actual-Predicted) %>%
  select(Model, Actual, Predicted, Residual)

# Create factors based on order of our methods
Results$Model = factor(Results$Model, levels = c("Naive Model", 
                                                 "Property Type Averages",
                                                 "Linear Regression", 
                                                 "Multiple Linear Regression",
                                                 "CART", 
                                                 "Random Forest"))

# Actual vs predicted by model with color as RMSE 
Results %>%
  ggplot(aes(x = Actual, y = Predicted, color = abs(Residual))) +
  geom_point(alpha = 0.8) +
  scale_color_gradient(low = "blue", high = "red") +
  geom_smooth(color = "black", size = .5) +
  coord_equal() +
  facet_wrap(~Model) +
  labs(color = "Residual",
       title = "Actual vs Predicted Values by Model")

```

Final table depicting the RMSE results for all of our tested methods and the final validation. 

```{r final table}

# This section presents the modeling results and discusses the model performance
# Final results table with test on validation set
results_tbl <- tibble(
  Method = c("Method #1", "Method #2", "Method #3", "Method #4", 
             "Method #5", "Method #6", "Validation"),
  Model = c("Naive Model", "Property Type EUI Averages", "LR - ENERGY STAR Score", 
            "Mulitple Linear Regression", "CART", "Random Forest", "Random Forest"),
  RMSE = c(rmse_naive, rmse_eui_avg, rmse_lr, rmse_mlr, 
           rmse_rpart, rmse_rf, rmse_validation)) %>%
  mutate(`RMSE Improvment` = scales::percent((RMSE-rmse_naive)/rmse_naive))

results_tbl %>%
  mutate(RMSE = round(RMSE, digits = 2)) %>%
  knitr::kable()

```

Examining the variable/feature importance in our final model - random forest. 

```{r rf importance}

importance_df <- varImp(model_rf) 
importance_df <- importance_df[["importance"]] %>%
  rownames_to_column(var = "Feature") 

importance_df %>%
  mutate(Feature = gsub(pattern = "primary_property_type", 
                        replacement = "Property Type = ",
                        x = Feature),
         Feature = fct_reorder(Feature, Overall)) %>%
  slice_max(order_by = Overall, n =10) %>%
  ggplot(aes(Feature, Overall)) +
  geom_col(fill = "lightblue", alpha = .9) +
  coord_flip() +
  theme_light() +
  labs(title = "Feature Importance in the Random Forest Model",
       subtitle = "[Top 10 Features, Scaled]")

```

## SECTION 5: Conclusion

insert text
